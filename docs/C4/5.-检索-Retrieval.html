
<!DOCTYPE HTML>
<html lang="" >
    <head>
        <meta charset="UTF-8">
        <title>5. 检索 Retrieval · HonKit</title>
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="description" content="">
        <meta name="generator" content="HonKit 5.1.4">
        
        
        
    
    <link rel="stylesheet" href="../gitbook/style.css">

    
            
                
                <link rel="stylesheet" href="../gitbook/@honkit/honkit-plugin-highlight/website.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-search/search.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-fontsettings/website.css">
                
            
        

    

    
        
    
        
    
        
    
        
    
        
    
        
    

        
    
    
    <meta name="HandheldFriendly" content="true"/>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=3, user-scalable=yes">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <link rel="apple-touch-icon-precomposed" sizes="152x152" href="../gitbook/images/apple-touch-icon-precomposed-152.png">
    <link rel="shortcut icon" href="../gitbook/images/favicon.ico" type="image/x-icon">

    
    <link rel="next" href="6.-问答-Question-Answering.html" />
    
    
    <link rel="prev" href="4.-向量数据库与词向量-Vectorstores-and-Embeddings.html" />
    

    </head>
    <body>
        
<div class="book honkit-cloak">
    <div class="book-summary">
        
            
<div id="book-search-input" role="search">
    <input type="text" placeholder="Type to search" />
</div>

            
                <nav role="navigation">
                


<ul class="summary">
    
    

    

    
        
        
    
        <li class="chapter " data-level="1.1" data-path="../">
            
                <a href="../">
            
                    
                    目录
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2" data-path="../前言.html">
            
                <a href="../前言.html">
            
                    
                    前言
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3" data-path="../环境配置.html">
            
                <a href="../环境配置.html">
            
                    
                    环境配置
            
                </a>
            

            
        </li>
    

    
        
        <li class="header">第一部分 面向开发者的提示工程</li>
        
        
    
        <li class="chapter " data-level="2.1" data-path="../C1/readme.html">
            
                <a href="../C1/readme.html">
            
                    
                    0. 概述
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="2.2" data-path="../C1/1.-简介-Introduction.html">
            
                <a href="../C1/1.-简介-Introduction.html">
            
                    
                    1. 简介 Introduction
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="2.3" data-path="../C1/2.-提示原则-Guidelines.html">
            
                <a href="../C1/2.-提示原则-Guidelines.html">
            
                    
                    2. 提示原则 Guidelines
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="2.4" data-path="../C1/3.-迭代优化-Iterative.html">
            
                <a href="../C1/3.-迭代优化-Iterative.html">
            
                    
                    3. 迭代优化 Iterative
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="2.5" data-path="../C1/4.-文本概括-Summarizing.html">
            
                <a href="../C1/4.-文本概括-Summarizing.html">
            
                    
                    4. 文本概括 Summarizing
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="2.6" data-path="../C1/5.-推断-Inferring.html">
            
                <a href="../C1/5.-推断-Inferring.html">
            
                    
                    5. 推断 Inferring
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="2.7" data-path="../C1/6.-文本转换-Transforming.html">
            
                <a href="../C1/6.-文本转换-Transforming.html">
            
                    
                    6. 文本转换 Transforming
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="2.8" data-path="../C1/7.-文本扩展-Expanding.html">
            
                <a href="../C1/7.-文本扩展-Expanding.html">
            
                    
                    7. 文本扩展 Expanding
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="2.9" data-path="../C1/8.-聊天机器人-Chatbot.html">
            
                <a href="../C1/8.-聊天机器人-Chatbot.html">
            
                    
                    8. 聊天机器人 Chatbot
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="2.10" data-path="../C1/9.-总结-Summary.html">
            
                <a href="../C1/9.-总结-Summary.html">
            
                    
                    9. 总结 Summary
            
                </a>
            

            
        </li>
    

    
        
        <li class="header">第二部分 搭建基于 ChatGPT 的问答系统</li>
        
        
    
        <li class="chapter " data-level="3.1" data-path="../C2/readme.html">
            
                <a href="../C2/readme.html">
            
                    
                    0. 概述
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="3.2" data-path="../C2/1.-简介-Introduction.html">
            
                <a href="../C2/1.-简介-Introduction.html">
            
                    
                    1. 简介 Introduction
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="3.3" data-path="../C2/2.-语言模型，提问范式与-Token-Language-Models,-the-Chat-Format-and-Tokens.html">
            
                <a href="../C2/2.-语言模型，提问范式与-Token-Language-Models,-the-Chat-Format-and-Tokens.html">
            
                    
                    2. 语言模型，提问范式与 Token Language Models, the Chat Format and Tokens
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="3.4" data-path="../C2/3.-评估输入-分类-Classification.html">
            
                <a href="../C2/3.-评估输入-分类-Classification.html">
            
                    
                    3. 评估输入-分类 Classification
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="3.5" data-path="../C2/4.-检查输入-监督-Moderation.html">
            
                <a href="../C2/4.-检查输入-监督-Moderation.html">
            
                    
                    4. 检查输入-监督 Moderation
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="3.6" data-path="../C2/5.-处理输入-思维链推理-Chain-of-Thought-Reasoning.html">
            
                <a href="../C2/5.-处理输入-思维链推理-Chain-of-Thought-Reasoning.html">
            
                    
                    5. 思维链推理 Chain of Thought Reasoning
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="3.7" data-path="../C2/6.-处理输入-链式-Prompt-Chaining-Prompts.html">
            
                <a href="../C2/6.-处理输入-链式-Prompt-Chaining-Prompts.html">
            
                    
                    6. Prompt 链 Chaining Prompts
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="3.8" data-path="../C2/7.-检查结果-Check-Outputs.html">
            
                <a href="../C2/7.-检查结果-Check-Outputs.html">
            
                    
                    7. 检查结果 Check Outputs
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="3.9" data-path="../C2/8.-搭建一个带评估的端到端问答系统-Evaluation.html">
            
                <a href="../C2/8.-搭建一个带评估的端到端问答系统-Evaluation.html">
            
                    
                    8. 搭建一个带评估的端到端系统 Evaluation
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="3.10" data-path="../C2/9.-评估（上）-Evaluation-part1.html">
            
                <a href="../C2/9.-评估（上）-Evaluation-part1.html">
            
                    
                    9. 评估（上）Evaluation-part1
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="3.11" data-path="../C2/10.-评估（下）Evaluation-part2.html">
            
                <a href="../C2/10.-评估（下）Evaluation-part2.html">
            
                    
                    10. 评估（下）Evaluation-part2
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="3.12" data-path="../C2/11.总结-conclusion.html">
            
                <a href="../C2/11.总结-conclusion.html">
            
                    
                    11. 总结 Conclusion
            
                </a>
            

            
        </li>
    

    
        
        <li class="header">第三部分 使用 LangChain 开发应用程序</li>
        
        
    
        <li class="chapter " data-level="4.1" data-path="../C3/readme.html">
            
                <a href="../C3/readme.html">
            
                    
                    0. 概述
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="4.2" data-path="../C3/1.-简介-Introduction.html">
            
                <a href="../C3/1.-简介-Introduction.html">
            
                    
                    1. 简介 Introduction
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="4.3" data-path="../C3/2.-模型、提示和解析器-Models,-Prompts-and-Output-Parsers.html">
            
                <a href="../C3/2.-模型、提示和解析器-Models,-Prompts-and-Output-Parsers.html">
            
                    
                    2. 模型、提示和解析器 Models, Prompts and Parsers
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="4.4" data-path="../C3/3.-储存-Memory.html">
            
                <a href="../C3/3.-储存-Memory.html">
            
                    
                    3. 储存 Memory
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="4.5" data-path="../C3/4.-模型链-Chains.html">
            
                <a href="../C3/4.-模型链-Chains.html">
            
                    
                    4. 模型链 Chains
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="4.6" data-path="../C3/5.-基于文档的问答-Question-and-Answer.html">
            
                <a href="../C3/5.-基于文档的问答-Question-and-Answer.html">
            
                    
                    5. 基于文档的问答 Question and Answer
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="4.7" data-path="../C3/6.-评估-Evaluation.html">
            
                <a href="../C3/6.-评估-Evaluation.html">
            
                    
                    6. 评估 Evaluation
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="4.8" data-path="../C3/7.-代理-Agent.html">
            
                <a href="../C3/7.-代理-Agent.html">
            
                    
                    7. 代理 Agent
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="4.9" data-path="../C3/8.-总结-Conclusion.html">
            
                <a href="../C3/8.-总结-Conclusion.html">
            
                    
                    8. 总结 Conclusion
            
                </a>
            

            
        </li>
    

    
        
        <li class="header">第四部分 使用 LangChain 访问个人数据</li>
        
        
    
        <li class="chapter " data-level="5.1" data-path="readme.html">
            
                <a href="readme.html">
            
                    
                    0. 概述
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="5.2" data-path="1.-简介-Introduction.html">
            
                <a href="1.-简介-Introduction.html">
            
                    
                    1. 简介 Introduction
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="5.3" data-path="2.-文档加载-Document-Loading.html">
            
                <a href="2.-文档加载-Document-Loading.html">
            
                    
                    2. 文档加载 Document Loading
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="5.4" data-path="3.-文档分割-Splitting.html">
            
                <a href="3.-文档分割-Splitting.html">
            
                    
                    3. 文档分割 Splitting
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="5.5" data-path="4.-向量数据库与词向量-Vectorstores-and-Embeddings.html">
            
                <a href="4.-向量数据库与词向量-Vectorstores-and-Embeddings.html">
            
                    
                    4. 向量数据库与词向量 Vectorstores and Embeddings
            
                </a>
            

            
        </li>
    
        <li class="chapter active" data-level="5.6" data-path="5.-检索-Retrieval.html">
            
                <a href="5.-检索-Retrieval.html">
            
                    
                    5. 检索 Retrieval
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="5.7" data-path="6.-问答-Question-Answering.html">
            
                <a href="6.-问答-Question-Answering.html">
            
                    
                    6. 问答 Question ANswering
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="5.8" data-path="7.-聊天-Chat.html">
            
                <a href="7.-聊天-Chat.html">
            
                    
                    7. 聊天 Chat
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="5.9" data-path="8.-总结-Summary.html">
            
                <a href="8.-总结-Summary.html">
            
                    
                    8. 总结 Summary
            
                </a>
            

            
        </li>
    

    

    <li class="divider"></li>

    <li>
        <a href="https://github.com/honkit/honkit" target="blank" class="gitbook-link">
            Published with HonKit
        </a>
    </li>
</ul>


                </nav>
            
        
    </div>

    <div class="book-body">
        
            <div class="body-inner">
                
                    

<div class="book-header" role="navigation">
    

    <!-- Title -->
    <h1>
        <i class="fa fa-circle-o-notch fa-spin"></i>
        <a href=".." >5. 检索 Retrieval</a>
    </h1>
</div>




                    <div class="page-wrapper" tabindex="-1" role="main">
                        <div class="page-inner">
                            
<div id="book-search-results">
    <div class="search-noresults">
    
                                <section class="normal markdown-section">
                                
                                <h1 id="第五章-检索retrieval">第五章 检索(Retrieval)</h1>
<p>在构建检索增强生成 (RAG) 系统时，信息检索是核心环节。检索模块负责对用户查询进行分析，从知识库中快速定位相关文档或段落，为后续的语言生成提供信息支持。<strong>检索是指根据用户的问题去向量数据库中搜索与问题相关的文档内容</strong>，当我们访问和查询向量数据库时可能会运用到如下几种技术：</p>
<ul>
<li>基本语义相似度(Basic semantic similarity)</li>
<li>最大边际相关性(Maximum marginal relevance，MMR)</li>
<li>过滤元数据</li>
<li>LLM辅助检索</li>
</ul>
<p><img src="../figures/C4/Retrieval.png" alt=""></img></p>
<div align="center"> 图 4.5.1 检索技术 </div>

<p>使用基本的相似性搜索大概能解决你80%的相关检索工作，但对于那些相似性搜索失败的边缘情况该如何解决呢？这一章节我们将介绍几种检索方法，以及解决检索边缘情况的技巧，让我们一起开始学习吧！</p>
<h2 id="一、向量数据库检索">一、向量数据库检索</h2>
<p>本章节需要使用<code>lark</code>包，若环境中未安装过此包，请运行以下命令安装：</p>
<pre><code class="lang-python">!pip install -Uq lark
</code></pre>
<h3 id="11-相似性检索（similarity-search）">1.1 相似性检索（Similarity Search）</h3>
<p>以我们的流程为例，前面课程已经存储了向量数据库(<code>VectorDB</code>)，包含各文档的语义向量表示。首先将上节课所保存的向量数据库(<code>VectorDB</code>)加载进来：</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> langchain.vectorstores <span class="hljs-keyword">import</span> Chroma
<span class="hljs-keyword">from</span> langchain.embeddings.openai <span class="hljs-keyword">import</span> OpenAIEmbeddings

persist_directory_chinese = <span class="hljs-string">'docs/chroma/matplotlib/'</span>

embedding = OpenAIEmbeddings()

vectordb_chinese = Chroma(
    persist_directory=persist_directory_chinese,
    embedding_function=embedding
)

<span class="hljs-built_in">print</span>(vectordb_chinese._collection.count())
</code></pre>
<pre><code>27
</code></pre><p>下面我们来实现一下语义的相似度搜索，我们把三句话存入向量数据库Chroma中，然后我们提出问题让向量数据库根据问题来搜索相关答案：</p>
<pre><code class="lang-python">texts_chinese = [
    <span class="hljs-string">"""毒鹅膏菌（Amanita phalloides）具有大型且引人注目的地上（epigeous）子实体（basidiocarp）"""</span>,
    <span class="hljs-string">"""一种具有大型子实体的蘑菇是毒鹅膏菌（Amanita phalloides）。某些品种全白。"""</span>,
    <span class="hljs-string">"""A. phalloides，又名死亡帽，是已知所有蘑菇中最有毒的一种。"""</span>,
]
</code></pre>
<p>我们可以看到前两句都是描述的是一种叫“鹅膏菌”的菌类，包括它们的特征：有较大的子实体，第三句描述的是“鬼笔甲”，一种已知的最毒的蘑菇，它的特征就是：含有剧毒。对于这个例子，我们将创建一个小数据库，我们可以作为一个示例来使用。</p>
<pre><code class="lang-python">smalldb_chinese = Chroma.from_texts(texts_chinese, embedding=embedding)
</code></pre>
<pre><code>100%|██████████| 1/1 [00:00&lt;00:00,  1.51it/s]
</code></pre><p>下面是我们对于这个示例所提出的问题：</p>
<pre><code class="lang-python">question_chinese = <span class="hljs-string">"告诉我关于具有大型子实体的全白色蘑菇的信息"</span>
</code></pre>
<p>现在，让针对上面问题进行<strong>相似性搜索</strong>，设置 k=2 ，只返回两个最相关的文档。</p>
<pre><code class="lang-python">smalldb_chinese.similarity_search(question_chinese, k=<span class="hljs-number">2</span>)
</code></pre>
<pre><code>[Document(page_content='一种具有大型子实体的蘑菇是毒鹅膏菌（Amanita phalloides）。某些品种全白。', metadata={}),
 Document(page_content='毒鹅膏菌（Amanita phalloides）具有大型且引人注目的地上（epigeous）子实体（basidiocarp）', metadata={})]
</code></pre><p>我们现在可以看到，向量数据库返回了 2 个文档，就是我们存入向量数据库中的第一句和第二句。这里我们很明显的就可以看到 chroma 的 similarity_search（相似性搜索） 方法可以根据问题的语义去数据库中搜索与之相关性最高的文档，也就是搜索到了第一句和第二句的文本。但这似乎还存在一些问题，因为第一句和第二句的含义非常接近，他们都是描述“鹅膏菌”及其“子实体”的，所以假如只返回其中的一句就足以满足要求了，如果返回两句含义非常接近的文本感觉是一种资源的浪费。下面我们来看一下 max_marginal_relevance_search 的搜索结果。</p>
<h3 id="12-解决多样性：最大边际相关性mmr">1.2 解决多样性：最大边际相关性(MMR)</h3>
<p>最大边际相关模型 (MMR，Maximal Marginal Relevance) 是实现多样性检索的常用算法。</p>
<p><img src="../figures/C4/MMR_algorithm.png" alt=""></img></p>
<div align="center"> 图 4.5.2 MMR </div>

<p><strong>MMR 的基本思想是同时考量查询与文档的相关度，以及文档之间的相似度</strong>。相关度确保返回结果对查询高度相关，相似度则鼓励不同语义的文档被包含进结果集。具体来说，它计算每个候选文档与查询的相关度，并减去与已经选入结果集的文档的相似度。这样更不相似的文档会有更高的得分。</p>
<p>总之，MMR 是解决检索冗余问题、提供多样性结果的一种简单高效的算法。它平衡了相关性和多样性，适用于对多样信息需求较强的应用场景。</p>
<p>我们来看一个利用 MMR 从蘑菇知识库中检索信息的示例。首先加载有关蘑菇的文档，然后运行 MMR 算法，设置 fetch_k 参数，用来告诉向量数据库我们最终需要 k 个结果返回。fetch_k=3 ，也就是我们最初获取 3 个文档，k=2 表示返回最不同的 2 个文档。</p>
<pre><code class="lang-python">smalldb_chinese.max_marginal_relevance_search(question_chinese,k=<span class="hljs-number">2</span>, fetch_k=<span class="hljs-number">3</span>)
</code></pre>
<pre><code>[Document(page_content='一种具有大型子实体的蘑菇是毒鹅膏菌（Amanita phalloides）。某些品种全白。', metadata={}),
 Document(page_content='A. phalloides，又名死亡帽，是已知所有蘑菇中最有毒的一种。', metadata={})]
</code></pre><p>这里我们看到 max_marginal_relevance_search（最大边际相关搜索） 返回了第二和第三句的文本，尽管第三句与我们的问题的相关性不太高，但是这样的结果其实应该是更加的合理，因为第一句和第二句文本本来就有着相似的含义，所以只需要返回其中的一句就可以了，另外再返回一个与问题相关性弱一点的答案(第三句文本)，这样似乎增强了答案的多样性，相信用户也会更加偏爱</p>
<p>还记得在上一节中我们介绍了两种向量数据在查询时的失败场景吗？当向量数据库中存在相同的文档时，而用户的问题又与这些重复的文档高度相关时，向量数据库会出现返回重复文档的情况。现在我们就可以运用Langchain的   max_marginal_relevance_search 来解决这个问题：</p>
<p>我们首先看看前两个文档，只看前几个字符，可以看到它们是相同的。</p>
<pre><code class="lang-python">question_chinese = <span class="hljs-string">"Matplotlib是什么？"</span>
docs_ss_chinese = vectordb_chinese.similarity_search(question_chinese,k=<span class="hljs-number">3</span>)

<span class="hljs-built_in">print</span>(<span class="hljs-string">"docs[0]: "</span>)
<span class="hljs-built_in">print</span>(docs_ss_chinese[<span class="hljs-number">0</span>].page_content[:<span class="hljs-number">100</span>])
<span class="hljs-built_in">print</span>()
<span class="hljs-built_in">print</span>(<span class="hljs-string">"docs[1]: "</span>)
<span class="hljs-built_in">print</span>(docs_ss_chinese[<span class="hljs-number">1</span>].page_content[:<span class="hljs-number">100</span>])
</code></pre>
<pre><code>docs[0]: 
第⼀回：Matplotlib 初相识
⼀、认识matplotlib
Matplotlib 是⼀个 Python 2D 绘图库，能够以多种硬拷⻉格式和跨平台的交互式环境⽣成出版物质量的图形，⽤来绘制各种

docs[1]: 
第⼀回：Matplotlib 初相识
⼀、认识matplotlib
Matplotlib 是⼀个 Python 2D 绘图库，能够以多种硬拷⻉格式和跨平台的交互式环境⽣成出版物质量的图形，⽤来绘制各种
</code></pre><p>这里如果我们使用相似查询，会得到两个重复的结果，读者可以自己尝试一下，这里不再展示。我们可以使用 <code>MMR</code> 得到不一样的结果。</p>
<pre><code class="lang-python">docs_mmr_chinese = vectordb_chinese.max_marginal_relevance_search(question_chinese,k=<span class="hljs-number">3</span>)
</code></pre>
<p>当我们运行 MMR 后得到结果时，我们可以看到第一个与之前的相同，因为那是最相似的。</p>
<pre><code class="lang-python"><span class="hljs-built_in">print</span>(docs_mmr_chinese[<span class="hljs-number">0</span>].page_content[:<span class="hljs-number">100</span>])
</code></pre>
<pre><code>第⼀回：Matplotlib 初相识
⼀、认识matplotlib
Matplotlib 是⼀个 Python 2D 绘图库，能够以多种硬拷⻉格式和跨平台的交互式环境⽣成出版物质量的图形，⽤来绘制各种
</code></pre><p>但是当我们进行到第二个时，我们可以看到它是不同的。</p>
<p>它在回应中获得了一些多样性。</p>
<pre><code class="lang-python"><span class="hljs-built_in">print</span>(docs_mmr_chinese[<span class="hljs-number">1</span>].page_content[:<span class="hljs-number">100</span>])
</code></pre>
<pre><code>By Datawhale 数据可视化开源⼩组
© Copyright © Copyright 2021.y轴分为左右两个，因此 tick1 对应左侧的轴； tick2 对应右侧的轴。
x轴分为上下两个
</code></pre><p>从以上结果中可以看到，向量数据库返回了 2 篇完全不同的文档，这是因为我们使用的是 MMR 搜索，它把搜索结果中相似度很高的文档做了过滤，所以它保留了结果的相关性又同时兼顾了结果的多样性。</p>
<h3 id="13-解决特殊性：使用元数据">1.3 解决特殊性：使用元数据</h3>
<p>在上一节课中，关于失败的应用场景我们还提出了一个问题，是询问了关于文档中某一讲的问题，但得到的结果中也包括了来自其他讲的结果。这是我们所不希望看到的结果，之所以产生这样的结果是因为当我们向向量数据库提出问题时，数据库并没有很好的理解问题的语义，所以返回的结果不如预期。要解决这个问题，我们可以通过过滤元数据的方式来实现精准搜索，当前很多向量数据库都支持对<code>元数据（metadata）</code>的操作：</p>
<p><code>metadata</code>为每个嵌入的块(embedded chunk)提供上下文。</p>
<pre><code class="lang-python">question_chinese = <span class="hljs-string">"他们在第二讲中对Figure说了些什么？"</span>
</code></pre>
<p>现在，我们以手动的方式来解决这个问题，我们会指定一个元数据过滤器<code>filter</code></p>
<pre><code class="lang-python">docs_chinese = vectordb_chinese.similarity_search(
    question_chinese,
    k=<span class="hljs-number">3</span>,
    <span class="hljs-built_in">filter</span>={<span class="hljs-string">"source"</span>:<span class="hljs-string">"docs/matplotlib/第二回：艺术画笔见乾坤.pdf"</span>}
)
</code></pre>
<p>接下来，我们可以看到结果都来自对应的章节</p>
<pre><code class="lang-python"><span class="hljs-keyword">for</span> d <span class="hljs-keyword">in</span> docs_chinese:
    <span class="hljs-built_in">print</span>(d.metadata)
</code></pre>
<pre><code>{'source': 'docs/matplotlib/第二回：艺术画笔见乾坤.pdf', 'page': 9}
{'source': 'docs/matplotlib/第二回：艺术画笔见乾坤.pdf', 'page': 10}
{'source': 'docs/matplotlib/第二回：艺术画笔见乾坤.pdf', 'page': 0}
</code></pre><p>当然，我们不能每次都采用手动的方式来解决这个问题，这会显得不够智能。下一小节中，我们将展示通过LLM来解决这个问题。</p>
<h3 id="14-解决特殊性：在元数据中使用自查询检索器（llm辅助检索）">1.4 解决特殊性：在元数据中使用自查询检索器（LLM辅助检索）</h3>
<p>在上例中，我们手动设置了过滤参数 filter 来过滤指定文档。但这种方式不够智能，需要人工指定过滤条件。如何自动从用户问题中提取过滤信息呢？</p>
<p>LangChain提供了SelfQueryRetriever模块，它可以通过语言模型从问题语句中分析出:</p>
<p>1) 向量搜索的查询字符串(search term)</p>
<p>2) 过滤文档的元数据条件(Filter)</p>
<p>以“除了维基百科,还有哪些健康网站”为例,SelfQueryRetriever可以推断出“除了维基百科”表示需要过滤的条件,即排除维基百科的文档。</p>
<p>它使用语言模型自动解析语句语义,提取过滤信息,无需手动设置。这种基于理解的元数据过滤更加智能方便,可以自动处理更复杂的过滤逻辑。</p>
<p>掌握利用语言模型实现自动化过滤的技巧,可以大幅降低构建针对性问答系统的难度。这种自抽取查询的方法使检索更加智能和动态。</p>
<p>其原理如下图所示：</p>
<p><img src="../figures/C4/LLM%20Aided%20Retrieval.png" alt=""></img></p>
<div align="center"> 图 4.5.3 自抽取查询 </div>


<p>下面我们就来实现一下LLM辅助检索：</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> langchain.llms <span class="hljs-keyword">import</span> OpenAI
<span class="hljs-keyword">from</span> langchain.retrievers.self_query.base <span class="hljs-keyword">import</span> SelfQueryRetriever
<span class="hljs-keyword">from</span> langchain.chains.query_constructor.base <span class="hljs-keyword">import</span> AttributeInfo

llm = OpenAI(temperature=<span class="hljs-number">0</span>)
</code></pre>
<p>这里我们首先定义了 metadata_field_info_chinese ，它包含了元数据的过滤条件 <code>source</code> 和 <code>page</code> , 其中 source 的作用是告诉 LLM 我们想要的数据来自于哪里， page 告诉 LLM 我们需要提取相关的内容在原始文档的哪一页。有了 metadata_field_info_chinese 信息后，LLM会自动从用户的问题中提取出上图中的 Filter 和 Search term 两项，然后向量数据库基于这两项去搜索相关的内容。下面我们看一下查询结果：</p>
<pre><code class="lang-python">metadata_field_info_chinese = [
    AttributeInfo(
        name=<span class="hljs-string">"source"</span>,
        description=<span class="hljs-string">"The lecture the chunk is from, should be one of `docs/matplotlib/第一回：Matplotlib初相识.pdf`, `docs/matplotlib/第二回：艺术画笔见乾坤.pdf`, or `docs/matplotlib/第三回：布局格式定方圆.pdf`"</span>,
        <span class="hljs-built_in">type</span>=<span class="hljs-string">"string"</span>,
    ),
    AttributeInfo(
        name=<span class="hljs-string">"page"</span>,
        description=<span class="hljs-string">"The page from the lecture"</span>,
        <span class="hljs-built_in">type</span>=<span class="hljs-string">"integer"</span>,
    ),
]

document_content_description_chinese = <span class="hljs-string">"Matplotlib 课堂讲义"</span>
retriever_chinese = SelfQueryRetriever.from_llm(
    llm,
    vectordb_chinese,
    document_content_description_chinese,
    metadata_field_info_chinese,
    verbose=<span class="hljs-literal">True</span>
)

question_chinese = <span class="hljs-string">"他们在第二讲中对Figure做了些什么？"</span>
</code></pre>
<p>当你第一次执行下一行时，你会收到关于predict_and_parse已被弃用的<strong>警告</strong>。 这可以安全地忽略。</p>
<pre><code class="lang-python">docs_chinese = retriever_chinese.get_relevant_documents(question_chinese)
</code></pre>
<pre><code>/root/autodl-tmp/env/gpt/lib/python3.10/site-packages/langchain/chains/llm.py:275: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.
  warnings.warn(


query='Figure' filter=Comparison(comparator=&lt;Comparator.EQ: 'eq'&gt;, attribute='source', value='docs/matplotlib/第二回：艺术画笔见乾坤.pdf') limit=None
</code></pre><p>打印可以看到查询结果，基于子查询检索器，我们检索到的结果都是在第二回的文档中：</p>
<pre><code class="lang-python"><span class="hljs-keyword">for</span> d <span class="hljs-keyword">in</span> docs_chinese:
    <span class="hljs-built_in">print</span>(d.metadata)
</code></pre>
<pre><code>{'source': 'docs/matplotlib/第二回：艺术画笔见乾坤.pdf', 'page': 9}
{'source': 'docs/matplotlib/第二回：艺术画笔见乾坤.pdf', 'page': 10}
{'source': 'docs/matplotlib/第二回：艺术画笔见乾坤.pdf', 'page': 0}
{'source': 'docs/matplotlib/第二回：艺术画笔见乾坤.pdf', 'page': 6}
</code></pre><h3 id="15-其他技巧：压缩">1.5 其他技巧：压缩</h3>
<p>在使用向量检索获取相关文档时，直接返回整个文档片段可能带来资源浪费，因为实际相关的只是文档的一小部分。为改进这一点，LangChain提供了一种“<code>压缩</code>”检索机制。其工作原理是，<strong>先使用标准向量检索获得候选文档，然后基于查询语句的语义，使用语言模型压缩这些文档,只保留与问题相关的部分</strong>。例如，对“蘑菇的营养价值”这个查询，检索可能返回整篇有关蘑菇的长文档。经压缩后，只提取文档中与“营养价值”相关的句子。</p>
<p><img src="../figures/C4/Compression.png" alt=""></img></p>
<div align="center"> 图 4.5.4 压缩 </div>

<p>从上图中我们看到，当向量数据库返回了所有与问题相关的所有文档块的全部内容后，会有一个Compression LLM来负责对这些返回的文档块的内容进行压缩，所谓压缩是指仅从文档块中提取出和用户问题相关的内容，并舍弃掉那些不相关的内容。</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> langchain.retrievers <span class="hljs-keyword">import</span> ContextualCompressionRetriever
<span class="hljs-keyword">from</span> langchain.retrievers.document_compressors <span class="hljs-keyword">import</span> LLMChainExtractor

<span class="hljs-keyword">def</span> <span class="hljs-title function_">pretty_print_docs</span>(<span class="hljs-params">docs</span>):
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f"\n<span class="hljs-subst">{<span class="hljs-string">'-'</span> * <span class="hljs-number">100</span>}</span>\n"</span>.join([<span class="hljs-string">f"Document <span class="hljs-subst">{i+<span class="hljs-number">1</span>}</span>:\n\n"</span> + d.page_content <span class="hljs-keyword">for</span> i, d <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(docs)]))

llm = OpenAI(temperature=<span class="hljs-number">0</span>)
compressor = LLMChainExtractor.from_llm(llm)  <span class="hljs-comment"># 压缩器</span>

compression_retriever_chinese = ContextualCompressionRetriever(
    base_compressor=compressor,
    base_retriever=vectordb_chinese.as_retriever()
)
<span class="hljs-comment"># 对源文档进行压缩</span>

question_chinese = <span class="hljs-string">"Matplotlib是什么？"</span>
compressed_docs_chinese = compression_retriever_chinese.get_relevant_documents(question_chinese)
pretty_print_docs(compressed_docs_chinese)
</code></pre>
<pre><code>Document 1:

Matplotlib 是⼀个 Python 2D 绘图库，能够以多种硬拷⻉格式和跨平台的交互式环境⽣成出版物质量的图形，⽤来绘制各种静态，动态，交互式的图表。
----------------------------------------------------------------------------------------------------
Document 2:

Matplotlib 是⼀个 Python 2D 绘图库，能够以多种硬拷⻉格式和跨平台的交互式环境⽣成出版物质量的图形，⽤来绘制各种静态，动态，交互式的图表。
</code></pre><p>在上面的代码中我们定义了一个 LLMChainExtractor ，它是一个压缩器，它负责从向量数据库返回的文档块中提取相关信息，然后我们还定义了 ContextualCompressionRetriever ，它有两个参数：base_compressor 和 base_retriever，其中 base_compressor 是我们前面定义的 LLMChainExtractor 的实例，base_retriever是早前定义的 vectordb 产生的检索器。</p>
<p>现在当我们提出问题后，查看结果文档，我们可以看到两件事。</p>
<ol>
<li>它们比正常文档短很多</li>
<li>仍然有一些重复的东西，这是因为在底层我们使用的是语义搜索算法。</li>
</ol>
<p>从上述例子中，我们可以发现这种压缩可以有效提升输出质量，同时节省通过长文档带来的计算资源浪费，降低成本。上下文相关的压缩检索技术，使得到的支持文档更严格匹配问题需求，是提升问答系统效率的重要手段。读者可以在实际应用中考虑这一技术。</p>
<h2 id="二、结合各种技术">二、结合各种技术</h2>
<p>为了去掉结果中的重复文档，我们在从向量数据库创建检索器时，可以将搜索类型设置为 MMR 。然后我们可以重新运行这个过程，可以看到我们返回的是一个过滤过的结果集，其中不包含任何重复的信息。</p>
<pre><code class="lang-python">compression_retriever_chinese = ContextualCompressionRetriever(
    base_compressor=compressor,
    base_retriever=vectordb_chinese.as_retriever(search_type = <span class="hljs-string">"mmr"</span>)
)

question_chinese = <span class="hljs-string">"Matplotlib是什么？"</span>
compressed_docs_chinese = compression_retriever_chinese.get_relevant_documents(question_chinese)
pretty_print_docs(compressed_docs_chinese)
</code></pre>
<pre><code>Document 1:

Matplotlib 是⼀个 Python 2D 绘图库，能够以多种硬拷⻉格式和跨平台的交互式环境⽣成出版物质量的图形，⽤来绘制各种静态，动态，交互式的图表。
</code></pre><h2 id="三、其他类型的检索">三、其他类型的检索</h2>
<p>值得注意的是，vetordb 并不是唯一一种检索文档的工具。<code>LangChain</code> 还提供了其他检索文档的方式，例如：<code>TF-IDF</code> 或 <code>SVM</code>。</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> langchain.retrievers <span class="hljs-keyword">import</span> SVMRetriever
<span class="hljs-keyword">from</span> langchain.retrievers <span class="hljs-keyword">import</span> TFIDFRetriever
<span class="hljs-keyword">from</span> langchain.document_loaders <span class="hljs-keyword">import</span> PyPDFLoader
<span class="hljs-keyword">from</span> langchain.text_splitter <span class="hljs-keyword">import</span> RecursiveCharacterTextSplitter

<span class="hljs-comment"># 加载PDF</span>
loader_chinese = PyPDFLoader(<span class="hljs-string">"docs/matplotlib/第一回：Matplotlib初相识.pdf"</span>)
pages_chinese = loader_chinese.load()
all_page_text_chinese = [p.page_content <span class="hljs-keyword">for</span> p <span class="hljs-keyword">in</span> pages_chinese]
joined_page_text_chinese = <span class="hljs-string">" "</span>.join(all_page_text_chinese)

<span class="hljs-comment"># 分割文本</span>
text_splitter_chinese = RecursiveCharacterTextSplitter(chunk_size = <span class="hljs-number">1500</span>,chunk_overlap = <span class="hljs-number">150</span>)
splits_chinese = text_splitter_chinese.split_text(joined_page_text_chinese)

<span class="hljs-comment"># 检索</span>
svm_retriever = SVMRetriever.from_texts(splits_chinese, embedding)
tfidf_retriever = TFIDFRetriever.from_texts(splits_chinese)
</code></pre>
<p>这里我们定义了 SVMRetriever ，和 TFIDFRetriever 两个检索器，接下来我们分别测试 TF-IDF 检索以及 SVM 检索的效果：</p>
<pre><code class="lang-python">question_chinese = <span class="hljs-string">"这门课的主要主题是什么？"</span> 
docs_svm_chinese = svm_retriever.get_relevant_documents(question_chinese)
<span class="hljs-built_in">print</span>(docs_svm_chinese[<span class="hljs-number">0</span>])
</code></pre>
<pre><code>page_content='fig, ax = plt.subplots()  \n# step4 绘制图像，  这⼀模块的扩展参考第⼆章进⼀步学习\nax.plot(x, y, label=\'linear\')  \n# step5 添加标签，⽂字和图例，这⼀模块的扩展参考第四章进⼀步学习\nax.set_xlabel(\'x label\') \nax.set_ylabel(\'y label\') \nax.set_title("Simple Plot")  \nax.legend() ;\n思考题\n请思考两种绘图模式的优缺点和各⾃适合的使⽤场景\n在第五节绘图模板中我们是以 OO 模式作为例⼦展示的，请思考并写⼀个 pyplot 绘图模式的简单模板' metadata={}
</code></pre><p>可以看出，SVM 检索的效果要差于 VectorDB。</p>
<pre><code class="lang-python">question_chinese = <span class="hljs-string">"Matplotlib是什么？"</span>
docs_tfidf_chinese = tfidf_retriever.get_relevant_documents(question_chinese)
<span class="hljs-built_in">print</span>(docs_tfidf_chinese[<span class="hljs-number">0</span>])
</code></pre>
<pre><code>page_content='fig, ax = plt.subplots()  \n# step4 绘制图像，  这⼀模块的扩展参考第⼆章进⼀步学习\nax.plot(x, y, label=\'linear\')  \n# step5 添加标签，⽂字和图例，这⼀模块的扩展参考第四章进⼀步学习\nax.set_xlabel(\'x label\') \nax.set_ylabel(\'y label\') \nax.set_title("Simple Plot")  \nax.legend() ;\n思考题\n请思考两种绘图模式的优缺点和各⾃适合的使⽤场景\n在第五节绘图模板中我们是以 OO 模式作为例⼦展示的，请思考并写⼀个 pyplot 绘图模式的简单模板' metadata={}
</code></pre><p>同样，TF-IDF 检索的效果也不尽如人意。</p>
<h2 id="四、总结">四、总结</h2>
<p>今天的课程涵盖了向量检索的多项新技术，让我们快速回顾关键要点：</p>
<p>1) MMR 算法可以实现兼具相关性与多样性的检索结果，避免信息冗余。</p>
<p>2) 定义元数据字段可以进行针对性过滤，提升匹配准确率。</p>
<p>3) SelfQueryRetriever 模块通过语言模型自动分析语句，提取查询字符串与过滤条件，无需手动设置，使检索更智能。</p>
<p>4) ContextualCompressionRetriever 实现压缩检索，仅返回与问题相关的文档片段，可以大幅提升效率并节省计算资源。</p>
<p>5) 除向量检索外，还简要介绍了基于 SVM 和 TF-IDF 的检索方法。</p>
<p>这些技术为我们构建可交互的语义搜索模块提供了重要支持。熟练掌握各检索算法的适用场景，将大大增强问答系统的智能水平。希望本节的教程能够对大家有所帮助!</p>
<h2 id="五、英文版">五、英文版</h2>
<p><strong>1.1 相似性检索</strong></p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> langchain.vectorstores <span class="hljs-keyword">import</span> Chroma
<span class="hljs-keyword">from</span> langchain.embeddings.openai <span class="hljs-keyword">import</span> OpenAIEmbeddings

persist_directory = <span class="hljs-string">'docs/chroma/cs229_lectures/'</span>

embedding = OpenAIEmbeddings()
vectordb = Chroma(
    persist_directory=persist_directory,
    embedding_function=embedding
)
<span class="hljs-built_in">print</span>(vectordb._collection.count())
</code></pre>
<pre><code>209
</code></pre><p>简单示例</p>
<pre><code class="lang-python">texts = [
    <span class="hljs-string">"""The Amanita phalloides has a large and imposing epigeous (aboveground) fruiting body (basidiocarp)."""</span>,
    <span class="hljs-string">"""A mushroom with a large fruiting body is the Amanita phalloides. Some varieties are all-white."""</span>,
    <span class="hljs-string">"""A. phalloides, a.k.a Death Cap, is one of the most poisonous of all known mushrooms."""</span>,
]

smalldb = Chroma.from_texts(texts, embedding=embedding)

question = <span class="hljs-string">"Tell me about all-white mushrooms with large fruiting bodies"</span>

<span class="hljs-built_in">print</span>(<span class="hljs-string">"相似性检索："</span>)
<span class="hljs-built_in">print</span>(smalldb.similarity_search(question, k=<span class="hljs-number">2</span>))
<span class="hljs-built_in">print</span>(<span class="hljs-string">"MMR 检索："</span>)
<span class="hljs-built_in">print</span>(smalldb_chinese.max_marginal_relevance_search(question,k=<span class="hljs-number">2</span>, fetch_k=<span class="hljs-number">3</span>))
</code></pre>
<pre><code>  0%|          | 0/1 [00:00&lt;?, ?it/s]100%|██████████| 1/1 [00:00&lt;00:00,  2.55it/s]


相似性检索：
[Document(page_content='A mushroom with a large fruiting body is the Amanita phalloides. Some varieties are all-white.', metadata={}), Document(page_content='The Amanita phalloides has a large and imposing epigeous (aboveground) fruiting body (basidiocarp).', metadata={})]
MMR 检索：
[Document(page_content='一种具有大型子实体的蘑菇是毒鹅膏菌（Amanita phalloides）。某些品种全白。', metadata={}), Document(page_content='A. phalloides，又名死亡帽，是已知所有蘑菇中最有毒的一种。', metadata={})]
</code></pre><p><strong>1.2 最大边际相关性</strong></p>
<pre><code class="lang-python">question = <span class="hljs-string">"what did they say about matlab?"</span>
docs_ss = vectordb.similarity_search(question,k=<span class="hljs-number">3</span>)

<span class="hljs-built_in">print</span>(<span class="hljs-string">"相似性检索："</span>)
<span class="hljs-built_in">print</span>(<span class="hljs-string">"docs[0]: "</span>)
<span class="hljs-built_in">print</span>(docs_ss[<span class="hljs-number">0</span>].page_content[:<span class="hljs-number">100</span>])
<span class="hljs-built_in">print</span>()
<span class="hljs-built_in">print</span>(<span class="hljs-string">"docs[1]: "</span>)
<span class="hljs-built_in">print</span>(docs_ss[<span class="hljs-number">1</span>].page_content[:<span class="hljs-number">100</span>])
<span class="hljs-built_in">print</span>()

docs_mmr = vectordb.max_marginal_relevance_search(question,k=<span class="hljs-number">3</span>)
<span class="hljs-built_in">print</span>(<span class="hljs-string">"MMR 检索："</span>)
<span class="hljs-built_in">print</span>(<span class="hljs-string">"mmr[0]: "</span>)
<span class="hljs-built_in">print</span>(docs_mmr[<span class="hljs-number">0</span>].page_content[:<span class="hljs-number">100</span>])
<span class="hljs-built_in">print</span>()
<span class="hljs-built_in">print</span>(<span class="hljs-string">"MMR 检索："</span>)
<span class="hljs-built_in">print</span>(<span class="hljs-string">"mmr[1]: "</span>)
<span class="hljs-built_in">print</span>(docs_mmr[<span class="hljs-number">1</span>].page_content[:<span class="hljs-number">100</span>])
</code></pre>
<pre><code>相似性检索：
docs[0]: 
those homeworks will be done in either MATLA B or in Octave, which is sort of — I 
know some people 

docs[1]: 
those homeworks will be done in either MATLA B or in Octave, which is sort of — I 
know some people 

MMR 检索：
mmr[0]: 
those homeworks will be done in either MATLA B or in Octave, which is sort of — I 
know some people 

MMR 检索：
mmr[1]: 
algorithm then? So what’s different? How come  I was making all that noise earlier about 
least squa
</code></pre><p><strong>1.3 使用元数据</strong></p>
<pre><code class="lang-python">question = <span class="hljs-string">"what did they say about regression in the third lecture?"</span>

docs = vectordb.similarity_search(
    question,
    k=<span class="hljs-number">3</span>,
    <span class="hljs-built_in">filter</span>={<span class="hljs-string">"source"</span>:<span class="hljs-string">"docs/cs229_lectures/MachineLearning-Lecture03.pdf"</span>}
)

<span class="hljs-keyword">for</span> d <span class="hljs-keyword">in</span> docs:
    <span class="hljs-built_in">print</span>(d.metadata)
</code></pre>
<pre><code>{'source': 'docs/cs229_lectures/MachineLearning-Lecture03.pdf', 'page': 0}
{'source': 'docs/cs229_lectures/MachineLearning-Lecture03.pdf', 'page': 14}
{'source': 'docs/cs229_lectures/MachineLearning-Lecture03.pdf', 'page': 4}
</code></pre><p><strong>1.4 使用自查询检索器</strong></p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> langchain.llms <span class="hljs-keyword">import</span> OpenAI
<span class="hljs-keyword">from</span> langchain.retrievers.self_query.base <span class="hljs-keyword">import</span> SelfQueryRetriever
<span class="hljs-keyword">from</span> langchain.chains.query_constructor.base <span class="hljs-keyword">import</span> AttributeInfo

llm = OpenAI(temperature=<span class="hljs-number">0</span>)

metadata_field_info = [
    AttributeInfo(
        name=<span class="hljs-string">"source"</span>,
        description=<span class="hljs-string">"The lecture the chunk is from, should be one of `docs/cs229_lectures/MachineLearning-Lecture01.pdf`, `docs/cs229_lectures/MachineLearning-Lecture02.pdf`, or `docs/cs229_lectures/MachineLearning-Lecture03.pdf`"</span>,
        <span class="hljs-built_in">type</span>=<span class="hljs-string">"string"</span>,
    ),
    AttributeInfo(
        name=<span class="hljs-string">"page"</span>,
        description=<span class="hljs-string">"The page from the lecture"</span>,
        <span class="hljs-built_in">type</span>=<span class="hljs-string">"integer"</span>,
    ),
]

document_content_description = <span class="hljs-string">"Lecture notes"</span>
retriever = SelfQueryRetriever.from_llm(
    llm,
    vectordb,
    document_content_description,
    metadata_field_info,
    verbose=<span class="hljs-literal">True</span>
)

question = <span class="hljs-string">"what did they say about regression in the third lecture?"</span>

docs = retriever.get_relevant_documents(question)

<span class="hljs-keyword">for</span> d <span class="hljs-keyword">in</span> docs:
    <span class="hljs-built_in">print</span>(d.metadata)
</code></pre>
<pre><code>/root/autodl-tmp/env/gpt/lib/python3.10/site-packages/langchain/chains/llm.py:275: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.
  warnings.warn(


query='regression' filter=Comparison(comparator=&lt;Comparator.EQ: 'eq'&gt;, attribute='source', value='docs/cs229_lectures/MachineLearning-Lecture03.pdf') limit=None
{'source': 'docs/cs229_lectures/MachineLearning-Lecture03.pdf', 'page': 14}
{'source': 'docs/cs229_lectures/MachineLearning-Lecture03.pdf', 'page': 0}
{'source': 'docs/cs229_lectures/MachineLearning-Lecture03.pdf', 'page': 10}
{'source': 'docs/cs229_lectures/MachineLearning-Lecture03.pdf', 'page': 10}
</code></pre><p><strong>1.5 压缩</strong></p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> langchain.retrievers <span class="hljs-keyword">import</span> ContextualCompressionRetriever
<span class="hljs-keyword">from</span> langchain.retrievers.document_compressors <span class="hljs-keyword">import</span> LLMChainExtractor

<span class="hljs-keyword">def</span> <span class="hljs-title function_">pretty_print_docs</span>(<span class="hljs-params">docs</span>):
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f"\n<span class="hljs-subst">{<span class="hljs-string">'-'</span> * <span class="hljs-number">100</span>}</span>\n"</span>.join([<span class="hljs-string">f"Document <span class="hljs-subst">{i+<span class="hljs-number">1</span>}</span>:\n\n"</span> + d.page_content <span class="hljs-keyword">for</span> i, d <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(docs)]))

llm = OpenAI(temperature=<span class="hljs-number">0</span>)
compressor = LLMChainExtractor.from_llm(llm)  <span class="hljs-comment"># 压缩器</span>

compression_retriever = ContextualCompressionRetriever(
    base_compressor=compressor,
    base_retriever=vectordb.as_retriever()
)

question = <span class="hljs-string">"what did they say about matlab?"</span>
compressed_docs = compression_retriever.get_relevant_documents(question)
pretty_print_docs(compressed_docs)
</code></pre>
<pre><code>Document 1:

"MATLAB is I guess part of the programming language that makes it very easy to write codes using matrices, to write code for numerical routines, to move data around, to plot data. And it's sort of an extremely easy to learn tool to use for implementing a lot of learning algorithms."
----------------------------------------------------------------------------------------------------
Document 2:

"MATLAB is I guess part of the programming language that makes it very easy to write codes using matrices, to write code for numerical routines, to move data around, to plot data. And it's sort of an extremely easy to learn tool to use for implementing a lot of learning algorithms."
----------------------------------------------------------------------------------------------------
Document 3:

"And the student said, "Oh, it was the MATLAB." So for those of you that don't know MATLAB yet, I hope you do learn it. It's not hard, and we'll actually have a short MATLAB tutorial in one of the discussion sections for those of you that don't know it."
----------------------------------------------------------------------------------------------------
Document 4:

"And the student said, "Oh, it was the MATLAB." So for those of you that don't know MATLAB yet, I hope you do learn it. It's not hard, and we'll actually have a short MATLAB tutorial in one of the discussion sections for those of you that don't know it."
</code></pre><p><strong>2.1 结合各种技术</strong></p>
<pre><code class="lang-python">compression_retriever = ContextualCompressionRetriever(
    base_compressor=compressor,
    base_retriever=vectordb.as_retriever(search_type = <span class="hljs-string">"mmr"</span>)
)

question = <span class="hljs-string">"what did they say about matlab?"</span>
compressed_docs = compression_retriever.get_relevant_documents(question)
pretty_print_docs(compressed_docs)
</code></pre>
<pre><code>Document 1:

"MATLAB is I guess part of the programming language that makes it very easy to write codes using matrices, to write code for numerical routines, to move data around, to plot data. And it's sort of an extremely easy to learn tool to use for implementing a lot of learning algorithms."
----------------------------------------------------------------------------------------------------
Document 2:

"And the student said, "Oh, it was the MATLAB." So for those of you that don't know MATLAB yet, I hope you do learn it. It's not hard, and we'll actually have a short MATLAB tutorial in one of the discussion sections for those of you that don't know it."
</code></pre><p><strong>3.1 其他类型的检索</strong></p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> langchain.retrievers <span class="hljs-keyword">import</span> SVMRetriever
<span class="hljs-keyword">from</span> langchain.retrievers <span class="hljs-keyword">import</span> TFIDFRetriever
<span class="hljs-keyword">from</span> langchain.document_loaders <span class="hljs-keyword">import</span> PyPDFLoader
<span class="hljs-keyword">from</span> langchain.text_splitter <span class="hljs-keyword">import</span> RecursiveCharacterTextSplitter

<span class="hljs-comment"># 加载PDF</span>
loader = PyPDFLoader(<span class="hljs-string">"docs/cs229_lectures/MachineLearning-Lecture01.pdf"</span>)
pages = loader.load()
all_page_text = [p.page_content <span class="hljs-keyword">for</span> p <span class="hljs-keyword">in</span> pages]
joined_page_text = <span class="hljs-string">" "</span>.join(all_page_text)

<span class="hljs-comment"># 分割文本</span>
text_splitter = RecursiveCharacterTextSplitter(chunk_size = <span class="hljs-number">1500</span>,chunk_overlap = <span class="hljs-number">150</span>)
splits = text_splitter.split_text(joined_page_text)

<span class="hljs-comment"># 检索</span>
svm_retriever = SVMRetriever.from_texts(splits, embedding)
tfidf_retriever = TFIDFRetriever.from_texts(splits)

question = <span class="hljs-string">"What are major topics for this class?"</span>  <span class="hljs-comment"># 这门课的主要主题是什么？</span>
<span class="hljs-built_in">print</span>(<span class="hljs-string">"SVM:"</span>)
docs_svm = svm_retriever.get_relevant_documents(question)
<span class="hljs-built_in">print</span>(docs_svm[<span class="hljs-number">0</span>])

question = <span class="hljs-string">"what did they say about matlab?"</span>
<span class="hljs-built_in">print</span>(<span class="hljs-string">"TF-IDF:"</span>)
docs_tfidf = tfidf_retriever.get_relevant_documents(question)
<span class="hljs-built_in">print</span>(docs_tfidf[<span class="hljs-number">0</span>])
</code></pre>
<pre><code>SVM:
page_content="let me just check what questions you have righ t now. So if there are no questions, I'll just \nclose with two reminders, which are after class today or as you start to talk with other \npeople in this class, I just encourage you again to start to form project partners, to try to \nfind project partners to do your project with. And also, this is a good time to start forming \nstudy groups, so either talk to your friends  or post in the newsgroup, but we just \nencourage you to try to star t to do both of those today, okay? Form study groups, and try \nto find two other project partners.  \nSo thank you. I'm looking forward to teaching this class, and I'll see you in a couple of \ndays.   [End of Audio]  \nDuration: 69 minutes" metadata={}
TF-IDF:
page_content="Saxena and Min Sun here did, wh ich is given an image like this, right? This is actually a \npicture taken of the Stanford campus. You can apply that sort of cl ustering algorithm and \ngroup the picture into regions. Let me actually blow that up so that you can see it more \nclearly. Okay. So in the middle, you see the lines sort of groupi ng the image together, \ngrouping the image into [inaudible] regions.  \nAnd what Ashutosh and Min did was they then  applied the learning algorithm to say can \nwe take this clustering and us e it to build a 3D model of the world? And so using the \nclustering, they then had a lear ning algorithm try to learn what the 3D structure of the \nworld looks like so that they could come up with a 3D model that you can sort of fly \nthrough, okay? Although many people used to th ink it's not possible to take a single \nimage and build a 3D model, but using a lear ning algorithm and that sort of clustering \nalgorithm is the first step. They were able to.  \nI'll just show you one more example. I like this  because it's a picture of Stanford with our \nbeautiful Stanford campus. So again, taking th e same sort of clustering algorithms, taking \nthe same sort of unsupervised learning algor ithm, you can group the pixels into different \nregions. And using that as a pre-processing step, they eventually built this sort of 3D model of Stanford campus in a single picture.  You can sort of walk  into the ceiling, look" metadata={}
</code></pre>
                                
                                </section>
                            
    </div>
    <div class="search-results">
        <div class="has-results">
            
            <h1 class="search-results-title"><span class='search-results-count'></span> results matching "<span class='search-query'></span>"</h1>
            <ul class="search-results-list"></ul>
            
        </div>
        <div class="no-results">
            
            <h1 class="search-results-title">No results matching "<span class='search-query'></span>"</h1>
            
        </div>
    </div>
</div>

                        </div>
                    </div>
                
            </div>

            
                
                <a href="4.-向量数据库与词向量-Vectorstores-and-Embeddings.html" class="navigation navigation-prev " aria-label="Previous page: 4. 向量数据库与词向量 Vectorstores and Embeddings">
                    <i class="fa fa-angle-left"></i>
                </a>
                
                
                <a href="6.-问答-Question-Answering.html" class="navigation navigation-next " aria-label="Next page: 6. 问答 Question ANswering">
                    <i class="fa fa-angle-right"></i>
                </a>
                
            
        
    </div>

    <script>
        var gitbook = gitbook || [];
        gitbook.push(function() {
            gitbook.page.hasChanged({"page":{"title":"5. 检索 Retrieval","level":"5.6","depth":1,"next":{"title":"6. 问答 Question ANswering","level":"5.7","depth":1,"path":"C4/6.-问答-Question-Answering.md","ref":"C4/6.-问答-Question-Answering.md","articles":[]},"previous":{"title":"4. 向量数据库与词向量 Vectorstores and Embeddings","level":"5.5","depth":1,"path":"C4/4.-向量数据库与词向量-Vectorstores-and-Embeddings.md","ref":"C4/4.-向量数据库与词向量-Vectorstores-and-Embeddings.md","articles":[]},"dir":"ltr"},"config":{"gitbook":"*","theme":"default","variables":{},"plugins":[],"pluginsConfig":{"highlight":{},"search":{},"lunr":{"maxIndexSize":1000000,"ignoreSpecialCharacters":false},"fontsettings":{"theme":"white","family":"sans","size":2},"theme-default":{"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"showLevel":false}},"structure":{"langs":"LANGS.md","readme":"README.md","glossary":"GLOSSARY.md","summary":"SUMMARY.md"},"pdf":{"pageNumbers":true,"fontSize":12,"fontFamily":"Arial","paperSize":"a4","chapterMark":"pagebreak","pageBreaksBefore":"/","margin":{"right":62,"left":62,"top":56,"bottom":56},"embedFonts":false},"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"}},"file":{"path":"C4/5.-检索-Retrieval.md","mtime":"2024-08-23T21:42:59.791Z","type":"markdown"},"gitbook":{"version":"5.1.4","time":"2024-08-23T22:20:29.853Z"},"basePath":"..","book":{"language":""}});
        });
    </script>
</div>

        
    <noscript>
        <style>
            .honkit-cloak {
                display: block !important;
            }
        </style>
    </noscript>
    <script>
        // Restore sidebar state as critical path for prevent layout shift
        function __init__getSidebarState(defaultValue){
            var baseKey = "";
            var key = baseKey + ":sidebar";
            try {
                var value = localStorage[key];
                if (value === undefined) {
                    return defaultValue;
                }
                var parsed = JSON.parse(value);
                return parsed == null ? defaultValue : parsed;
            } catch (e) {
                return defaultValue;
            }
        }
        function __init__restoreLastSidebarState() {
            var isMobile = window.matchMedia("(max-width: 600px)").matches;
            if (isMobile) {
                // Init last state if not mobile
                return;
            }
            var sidebarState = __init__getSidebarState(true);
            var book = document.querySelector(".book");
            // Show sidebar if it enabled
            if (sidebarState && book) {
                book.classList.add("without-animation", "with-summary");
            }
        }

        try {
            __init__restoreLastSidebarState();
        } finally {
            var book = document.querySelector(".book");
            book.classList.remove("honkit-cloak");
        }
    </script>
    <script src="../gitbook/gitbook.js"></script>
    <script src="../gitbook/theme.js"></script>
    
        
        <script src="../gitbook/gitbook-plugin-search/search-engine.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-search/search.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-lunr/lunr.min.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-lunr/search-lunr.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-fontsettings/fontsettings.js"></script>
        
    

    </body>
</html>

